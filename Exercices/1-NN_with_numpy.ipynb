{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building a NN using numpy\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x using Euclidean error.\n",
    "\n",
    "The model that we want to build has the following structure:\n",
    "$$\\hat{y}(x) = \\text{relu}(x.w_1).w_2,$$\n",
    "where $x$ and $y$ are the input and output features (of dimension 1000 and 10, respectively). Here the relu activation function is used and $w_1$ and $w_2$ are weight matrices.\n",
    "\n",
    "This implementation uses numpy to manually compute the forward pass, loss, and\n",
    "backward pass. A numpy array is a generic n-dimensional array; it does not know anything about\n",
    "deep learning or gradients or computational graphs, and is just a way to perform\n",
    "generic numeric computations.\n",
    "\n",
    "This example is adapted from: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backward pass\n",
    "\n",
    "Given the forward pass: $x \\rightarrow h= x.w_1 \\rightarrow \\hat{y} = \\text{relu}(h).w_2$ and the following cost and loss functions:\n",
    "* Cost: $E(W) = \\sum_{i=1}^N (\\hat{y} - y)^2$\n",
    "* $\\text{loss}: \\ell(\\hat{y},W) = (\\hat{y} - y)^2,$\n",
    "\n",
    "calculate (analytically) the derivatives of loss function $\\frac{\\partial \\ell}{\\partial w_2}$ and $\\frac{\\partial \\ell}{\\partial w_1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of data and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 64      # N: input batch size\n",
    "D_in = 1000 # D_in: input dimension\n",
    "H = 100     # H: hidden layer dimension;\n",
    "D_out = 10  # D_out: output dimension\n",
    "\n",
    "# Create random input and output (target) training data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights (no bias terms)\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's look at the dimension of data and weights matrices\n",
    "For this use e.g. print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass: compute predicted y\n",
    "\n",
    "We want to compute: $\\hat{y} = \\text{relu}(x.w_1).w_2$, where the relu activation function is used.\n",
    "\n",
    "For this calculate (be careful of the matrix dimensions):\n",
    "* h = dot product of x and w1 (use .dot() function)\n",
    "* h_relu: $\\text{relu}(h)$ relu activation function (using np.maximum() function)\n",
    "* compute $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = x.dot(w1)       # .dot() = matrix multiplication               \n",
    "h_relu = np.maximum(# FILL HERE #)    \n",
    "y_pred = h_relu.dot(# FILL HERE #)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and print the cost function\n",
    "* Cost: $E(w_1,w_2) = \\sum_{i=1}^N (\\hat{y_i} - y_i)^2$\n",
    "* $\\text{loss}: \\ell(\\hat{y},W) = (\\hat{y} - y)^2.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "Compute gradients of $w_1$ and $w_2$ with respect to loss.\n",
    "\n",
    "Beware of matrices dimensions !\n",
    "\n",
    "Hint: use .dot() for matrix multiplication and .T to take the transpose of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_y_pred =   # FILL HERE #\n",
    "grad_w2 =       # FILL HERE #\n",
    "grad_h_relu =   # FILL HERE #\n",
    "grad_h = grad_h_relu.copy()\n",
    "grad_h[# CONDITION #] = # FILL VALUE #   # Here we use this trick to change grad_h values when a condition is realized\n",
    "grad_w1 =       # FILL HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update weights $w_1$ and $w_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat procedure 500 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "niteration = 500\n",
    "for t in range(niteration):\n",
    "    # COPY ABOVE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model performance (optional)\n",
    "Using matplotlib plot the evolution of cost as a function of the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
